{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9R-r4ksgMrZ"
   },
   "source": [
    "# Clustering and Topic Modeling of Student Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLYgIAlhAaTT"
   },
   "source": [
    "This notebook demonstrates how to cluster and explore **student questions** using embeddings, dimensionality reduction, clustering, and topic modeling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU9TWwjHXRFR"
   },
   "source": [
    "We will follow these steps:\n",
    "\n",
    "1. Convert questions into embedding vectors  \n",
    "2. Reduce the embeddings to 5 dimensions  \n",
    "3. Cluster the reduced embeddings  \n",
    "4. Generate keyword-based topics  \n",
    "5. Reduce embeddings to 2 dimensions for visualization  \n",
    "6. Visualize questions + topics  \n",
    "7. Use OpenAI to generate short topic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "id": "rKPJrZSPBwai",
    "outputId": "63d22743-38b0-4956-ca0d-3292f1225d3c"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload the local dataset file (choose the latest extracted_user_inputs_<date>.txt)\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded file name\n",
    "filename = list(uploaded.keys())[0]\n",
    "\n",
    "# Load questions from the uploaded file\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    questions = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(questions)} questions\")\n",
    "print(\"Sample:\", questions[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-7lZd1LFX1a"
   },
   "source": [
    "## Create Embeddings\n",
    "We will use a **sentence transformer** to embed each question into a high-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "60e546b611694f3f8afbe3618da0a73b",
      "c8b921a1591c4b5eac40f60d88d61c13",
      "ba7fa2d943dd4b5e92399bab17fd6130",
      "806b413bf955483393b3e1c23e3fd3af",
      "b2e04707dd2240bb806a78d8ffa9d9bb",
      "ff62af236d7847f5b48f158e2db4ce36",
      "30980c5f49d745c3b7b6ab08364c3141",
      "a18a941503834394a0f652f841d53515",
      "b8aa840aff79472fb1c2ec663eceb6f6",
      "04989cc1d1624c1c8e072e10913af90a",
      "4ecfe09d11ab47fe9be38892c1d79ded"
     ]
    },
    "id": "cQsl6CnuFksu",
    "outputId": "369e83dd-1bbf-4aa6-db21-066c09dfc1cb"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from google.colab import userdata\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Use Colab's secrets manager for API key\n",
    "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
    "\n",
    "# Function to generate embeddings in batches\n",
    "def generate_embeddings_in_batches(client, questions, model=\"text-embedding-3-small\", batch_size=1000):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(questions), batch_size)):\n",
    "        batch = questions[i : i + batch_size]\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=model,\n",
    "                input=batch\n",
    "            )\n",
    "            embeddings.extend([d.embedding for d in response.data])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size}: {e}\")\n",
    "            # Depending on the error, you might want to implement retry logic or skip the batch\n",
    "            continue\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings with OpenAI in batches\n",
    "embeddings = generate_embeddings_in_batches(client, questions)\n",
    "\n",
    "# Check the dimensions of the resulting embeddings\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz2uHQrJJoBC"
   },
   "source": [
    "## Reduce embeddings to 5 dimensions (for clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spago-9OJuiZ",
    "outputId": "864139ac-4b58-40a0-8b39-1d0f019f93a3"
   },
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# Reduce from 1536 ‚Üí 5 dims\n",
    "umap_model = UMAP(\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAHLR6GoKcz7"
   },
   "source": [
    "## Cluster the reduced embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuFM9aYZKhCs",
    "outputId": "25b03567-e99b-4198-b75a-e0934d90eb64"
   },
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Cluster with HDBSCAN\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=15, # tweak if clusters are too small/too many\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\"\n",
    ")\n",
    "clusters = hdbscan_model.fit_predict(reduced_embeddings)\n",
    "\n",
    "# How many clusters were found?\n",
    "unique_clusters, counts = np.unique(clusters, return_counts=True)\n",
    "print(f\"Number of clusters found: {len(unique_clusters)}\")\n",
    "\n",
    "# Calculate and print the number of clustered and unclustered questions\n",
    "unclustered_count = counts[unique_clusters == -1][0] if -1 in unique_clusters else 0\n",
    "clustered_count = len(clusters) - unclustered_count\n",
    "\n",
    "print(f\"Number of questions clustered: {clustered_count}\")\n",
    "print(f\"Number of questions not clustered: {unclustered_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCYd7YhRO98x",
    "outputId": "d54bf908-51d5-4575-db1d-cc197544f273"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reduce again for visualization (1536 ‚Üí 2)\n",
    "reduced_2d = UMAP(\n",
    "    n_components=2,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ").fit_transform(embeddings)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(reduced_2d, columns=[\"x\", \"y\"])\n",
    "df[\"questions\"] = questions\n",
    "df[\"cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "dehNLCfuQmwL",
    "outputId": "d8cadabb-e4b5-4d2d-fdbe-56fb7ecdd977"
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Train our model with our previously defined models\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=None, # We already have embeddings\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(questions, embeddings)\n",
    "\n",
    "topic_model.get_topic_info().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "cfiJ0uH5UV0y",
    "outputId": "465f4411-e1a9-440f-da61-3b7967f89761"
   },
   "outputs": [],
   "source": [
    "from bertopic.representation import OpenAI\n",
    "\n",
    "prompt = \"\"\"\n",
    "I have a topic that contains the following missionary questions:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the information above, extract a short topic label in the format:\n",
    "topic: <short topic label>\n",
    "\"\"\"\n",
    "\n",
    "representation_model = OpenAI(\n",
    "    client,\n",
    "    model=\"gpt-4o\", # Corrected model name\n",
    "    exponential_backoff=True,\n",
    "    chat=True,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "topic_model.update_topics(questions, representation_model=representation_model)\n",
    "topic_model.get_topic_info().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqELYYerV-Db"
   },
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHffkZXpWBz7",
    "outputId": "e117f656-802a-4561-e8c6-f0e25372cbc1"
   },
   "outputs": [],
   "source": [
    "# Interactive doc visualization\n",
    "fig = topic_model.visualize_documents(\n",
    "    questions,\n",
    "    reduced_embeddings=reduced_2d,\n",
    "    width=1200,\n",
    "    hide_annotations=True\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Keyword barchart\n",
    "topic_model.visualize_barchart()\n",
    "\n",
    "# Heatmap of topics\n",
    "topic_model.visualize_heatmap\n",
    "\n",
    "# Hierarchical topic structure\n",
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ba21af14",
    "outputId": "0b31f53d-786e-4503-968d-6e1d1eeaacc9"
   },
   "outputs": [],
   "source": [
    "# Get topic information\n",
    "# This calls the get_topic_info() method on the topic_model object\n",
    "# This method returns a pandas DataFrame containing detailed information about each topic\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Display the topic information\n",
    "# This uses the display() function to show the contents of the topic_info DataFrame\n",
    "# display() is often used in environments like Colab to render DataFrames in a more readable, formatted way\n",
    "display(topic_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Question-Topic Dataset (Using Representations)\n",
    "\n",
    "Now we'll create a dataset that maps each question to its cluster-generated topic representation. This will be our \"ground truth\" for testing OpenAI's classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from cluster ID to topic representation (not name)\n",
    "# Note: representations are stored as lists, so we take the first element\n",
    "topic_map = topic_info.set_index(\"Topic\")[\"Representation\"].to_dict()\n",
    "\n",
    "# Create a function to get the first representation from the list\n",
    "def get_topic_representation(cluster_id):\n",
    "    if cluster_id in topic_map:\n",
    "        representation = topic_map[cluster_id]\n",
    "        # Representation is stored as a list, get the first element\n",
    "        if isinstance(representation, list) and len(representation) > 0:\n",
    "            return representation[0]\n",
    "        else:\n",
    "            return str(representation)\n",
    "    else:\n",
    "        return \"Other\"  # For unclustered questions (-1)\n",
    "\n",
    "# Create DataFrame with questions and their cluster-assigned topic representations\n",
    "df_questions_topics = pd.DataFrame({\n",
    "    'question': questions,\n",
    "    'cluster': topics,  # This comes from topic_model.fit_transform\n",
    "    'cluster_topic': [get_topic_representation(cluster_id) for cluster_id in topics]\n",
    "})\n",
    "\n",
    "# Remove questions that were not clustered (cluster = -1)\n",
    "df_questions_topics = df_questions_topics[df_questions_topics['cluster'] != -1]\n",
    "\n",
    "print(f\"Total questions with assigned topics: {len(df_questions_topics)}\")\n",
    "print(f\"Number of unique topics: {len(df_questions_topics['cluster_topic'].unique())}\")\n",
    "print(\"\\nSample data:\")\n",
    "display(df_questions_topics.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Classification Testing Results\n",
    "\n",
    "The comprehensive model testing above shows the performance of different OpenAI models on the classification task. The testing uses an enhanced approach that forces models to select from available topics rather than defaulting to \"Other\".\n",
    "\n",
    "**Testing Summary**: \n",
    "- ‚úÖ **Model Comparison**: All available OpenAI models tested on {MODEL_COMPARISON_SAMPLE_SIZE} questions\n",
    "- ‚úÖ **Best Model Testing**: Detailed test with {BEST_MODEL_TEST_SIZE} questions using the top performer  \n",
    "- ‚úÖ **Full Dataset**: Optional testing on all {len(df_questions_topics)} questions (configurable)\n",
    "\n",
    "**Goal**: Achieve 90%+ accuracy to justify replacing clustering with direct OpenAI classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Settings\n",
    "\n",
    "Set up testing parameters and model configurations for easy customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# TESTING CONFIGURATION - Change these variables to customize testing\n",
    "# ====================================================================\n",
    "\n",
    "# Number of questions for initial model comparison (smaller sample)\n",
    "MODEL_COMPARISON_SAMPLE_SIZE = 20\n",
    "\n",
    "# Number of questions for best model testing (medium sample)\n",
    "BEST_MODEL_TEST_SIZE = 100\n",
    "\n",
    "# Whether to run full dataset test (all questions) - Set to True/False\n",
    "RUN_FULL_DATASET_TEST = False  # Change to True to test all questions\n",
    "\n",
    "# Concurrency settings for API calls\n",
    "OPTIMAL_CONCURRENCY = 16  # Recommended for stability\n",
    "HIGH_CONCURRENCY = 32     # For faster processing (may hit rate limits)\n",
    "\n",
    "print(f\"üìã TESTING CONFIGURATION:\")\n",
    "print(f\"   Model comparison sample: {MODEL_COMPARISON_SAMPLE_SIZE} questions\")\n",
    "print(f\"   Best model test: {BEST_MODEL_TEST_SIZE} questions\") \n",
    "print(f\"   Full dataset test: {'ENABLED' if RUN_FULL_DATASET_TEST else 'DISABLED'}\")\n",
    "print(f\"   Concurrency setting: {OPTIMAL_CONCURRENCY} parallel requests\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# OPENAI MODEL DATABASE - Complete model specifications and pricing\n",
    "# ====================================================================\n",
    "\n",
    "# Model specifications with accurate pricing (as of December 2025)\n",
    "OPENAI_MODELS = {\n",
    "    # GPT-5 Series (Latest - Released August 2025)\n",
    "    \"gpt-5\": {\n",
    "        \"name\": \"GPT-5\",\n",
    "        \"description\": \"Our smartest, fastest, most useful model yet\",\n",
    "        \"input_price\": 1.25,      # $1.25 per 1M tokens\n",
    "        \"output_price\": 10.00,    # $10.00 per 1M tokens\n",
    "        \"context_window\": 400000,\n",
    "        \"api_params\": {\"max_completion_tokens\": True, \"temperature_required\": False}\n",
    "    },\n",
    "    \"gpt-5-mini\": {\n",
    "        \"name\": \"GPT-5 Mini\", \n",
    "        \"description\": \"Faster, cheaper version of GPT-5 for well-defined tasks\",\n",
    "        \"input_price\": 0.25,      # $0.25 per 1M tokens\n",
    "        \"output_price\": 2.00,     # $2.00 per 1M tokens\n",
    "        \"context_window\": 400000,\n",
    "        \"api_params\": {\"max_completion_tokens\": True, \"temperature_required\": False}\n",
    "    },\n",
    "    \"gpt-5-nano\": {\n",
    "        \"name\": \"GPT-5 Nano\",\n",
    "        \"description\": \"Fastest, cheapest GPT-5 - optimized for classification\",\n",
    "        \"input_price\": 0.05,      # $0.05 per 1M tokens\n",
    "        \"output_price\": 0.40,     # $0.40 per 1M tokens  \n",
    "        \"context_window\": 400000,\n",
    "        \"api_params\": {\"max_completion_tokens\": True, \"temperature_required\": False}\n",
    "    },\n",
    "    \n",
    "    # GPT-4o Series (Current generation)\n",
    "    \"gpt-4o\": {\n",
    "        \"name\": \"GPT-4o\",\n",
    "        \"description\": \"High-performance multimodal model\",\n",
    "        \"input_price\": 2.50,      # Estimated pricing\n",
    "        \"output_price\": 10.00,    # Estimated pricing\n",
    "        \"context_window\": 128000,\n",
    "        \"api_params\": {\"max_completion_tokens\": False, \"temperature_required\": True}\n",
    "    },\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"name\": \"GPT-4o Mini\",\n",
    "        \"description\": \"Efficient and fast, cost-effective option\",\n",
    "        \"input_price\": 0.15,      # $0.15 per 1M tokens\n",
    "        \"output_price\": 0.60,     # $0.60 per 1M tokens\n",
    "        \"context_window\": 128000,\n",
    "        \"api_params\": {\"max_completion_tokens\": False, \"temperature_required\": True}\n",
    "    },\n",
    "    \"gpt-4o-2024-08-06\": {\n",
    "        \"name\": \"GPT-4o (Aug 2024)\",\n",
    "        \"description\": \"Specific snapshot version of GPT-4o\",\n",
    "        \"input_price\": 2.50,      # Estimated pricing\n",
    "        \"output_price\": 10.00,    # Estimated pricing  \n",
    "        \"context_window\": 128000,\n",
    "        \"api_params\": {\"max_completion_tokens\": False, \"temperature_required\": True}\n",
    "    },\n",
    "    \n",
    "    # GPT-4 Series (Previous generation)\n",
    "    \"gpt-4-turbo\": {\n",
    "        \"name\": \"GPT-4 Turbo\",\n",
    "        \"description\": \"Previous generation flagship model\",\n",
    "        \"input_price\": 10.00,     # $10.00 per 1M tokens\n",
    "        \"output_price\": 30.00,    # $30.00 per 1M tokens\n",
    "        \"context_window\": 128000,\n",
    "        \"api_params\": {\"max_completion_tokens\": False, \"temperature_required\": True}\n",
    "    },\n",
    "    \"gpt-4\": {\n",
    "        \"name\": \"GPT-4\",\n",
    "        \"description\": \"Original GPT-4 model\",\n",
    "        \"input_price\": 30.00,     # $30.00 per 1M tokens\n",
    "        \"output_price\": 60.00,    # $60.00 per 1M tokens\n",
    "        \"context_window\": 8192,\n",
    "        \"api_params\": {\"max_completion_tokens\": False, \"temperature_required\": True}\n",
    "    },\n",
    "    \n",
    "    # GPT-3.5 Series (Older generation)\n",
    "    \"gpt-3.5-turbo\": {\n",
    "        \"name\": \"GPT-3.5 Turbo\", \n",
    "        \"description\": \"Fast and affordable legacy model\",\n",
    "        \"input_price\": 0.50,      # $0.50 per 1M tokens\n",
    "        \"output_price\": 1.50,     # $1.50 per 1M tokens\n",
    "        \"context_window\": 16385,\n",
    "        \"api_params\": {\"max_completion_tokens\": False, \"temperature_required\": True}\n",
    "    },\n",
    "    \"gpt-3.5-turbo-0125\": {\n",
    "        \"name\": \"GPT-3.5 Turbo (Jan 2024)\",\n",
    "        \"description\": \"Specific snapshot of GPT-3.5 Turbo\",\n",
    "        \"input_price\": 0.50,      # $0.50 per 1M tokens\n",
    "        \"output_price\": 1.50,     # $1.50 per 1M tokens\n",
    "        \"context_window\": 16385, \n",
    "        \"api_params\": {\"max_completion_tokens\": False, \"temperature_required\": True}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print model database\n",
    "print(\"üîç OPENAI MODEL DATABASE\")\n",
    "print(\"=\"*80)\n",
    "for model_id, info in OPENAI_MODELS.items():\n",
    "    print(f\"üìã {info['name']} ({model_id})\")\n",
    "    print(f\"   üí∞ Pricing: ${info['input_price']:.2f} input / ${info['output_price']:.2f} output per 1M tokens\")\n",
    "    print(f\"   üìù Description: {info['description']}\")\n",
    "    print(f\"   üìä Context: {info['context_window']:,} tokens\")\n",
    "    print()\n",
    "\n",
    "print(f\"‚úÖ Total models configured: {len(OPENAI_MODELS)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# INTELLIGENT MODEL CONFIGURATION SYSTEM\n",
    "# ====================================================================\n",
    "\n",
    "def get_model_config(model_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get the appropriate API configuration for different OpenAI models.\n",
    "    Handles differences between GPT-5 and older models.\n",
    "    \"\"\"\n",
    "    if model_name in OPENAI_MODELS:\n",
    "        model_info = OPENAI_MODELS[model_name]\n",
    "        api_params = model_info[\"api_params\"]\n",
    "        \n",
    "        config = {\n",
    "            \"model\": model_name,\n",
    "            \"pricing\": {\n",
    "                \"input\": model_info[\"input_price\"],\n",
    "                \"output\": model_info[\"output_price\"]\n",
    "            },\n",
    "            \"context_window\": model_info[\"context_window\"],\n",
    "            \"name\": model_info[\"name\"]\n",
    "        }\n",
    "        \n",
    "        # Configure API parameters based on model type\n",
    "        if api_params[\"max_completion_tokens\"]:\n",
    "            # GPT-5 series uses max_completion_tokens\n",
    "            config[\"max_tokens_param\"] = \"max_completion_tokens\"\n",
    "            config[\"max_tokens_value\"] = 200  # Sufficient for topic names without truncation\n",
    "            # FIX: GPT-5 models work better with explicit temperature\n",
    "            config[\"temperature\"] = 0.1  # Low but consistent temperature for all models\n",
    "        else:\n",
    "            # GPT-4 and older use max_tokens\n",
    "            config[\"max_tokens_param\"] = \"max_tokens\"\n",
    "            config[\"max_tokens_value\"] = 200  # Increased from 100 for better responses\n",
    "            config[\"temperature\"] = 0.1  # Consistent temperature across all models\n",
    "            \n",
    "        return config\n",
    "    else:\n",
    "        # Default configuration for unknown models\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"max_tokens_param\": \"max_tokens\",\n",
    "            \"max_tokens_value\": 100,\n",
    "            \"temperature\": 0,\n",
    "            \"pricing\": {\"input\": 0.0, \"output\": 0.0},\n",
    "            \"context_window\": 8192,\n",
    "            \"name\": model_name\n",
    "        }\n",
    "\n",
    "def estimate_cost(input_tokens: int, output_tokens: int, model_name: str) -> float:\n",
    "    \"\"\"Calculate estimated cost for API calls\"\"\"\n",
    "    if model_name in OPENAI_MODELS:\n",
    "        info = OPENAI_MODELS[model_name]\n",
    "        input_cost = (input_tokens / 1_000_000) * info[\"input_price\"]\n",
    "        output_cost = (output_tokens / 1_000_000) * info[\"output_price\"]\n",
    "        return input_cost + output_cost\n",
    "    return 0.0\n",
    "\n",
    "# Test the configuration system\n",
    "print(\"üîß TESTING MODEL CONFIGURATION SYSTEM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_models = [\"gpt-5-nano\", \"gpt-4o-mini\", \"gpt-3.5-turbo\"]\n",
    "for model in test_models:\n",
    "    config = get_model_config(model)\n",
    "    print(f\"üìã {config['name']}:\")\n",
    "    print(f\"   üîß API param: {config['max_tokens_param']} = {config['max_tokens_value']}\")\n",
    "    print(f\"   üå°Ô∏è Temperature: {config['temperature']}\")\n",
    "    print(f\"   üí∞ Pricing: ${config['pricing']['input']:.2f}/${config['pricing']['output']:.2f} per 1M tokens\")\n",
    "    \n",
    "    # Estimate cost for 100 questions (rough estimate)\n",
    "    est_cost = estimate_cost(50000, 5000, model)  # ~500 tokens per question\n",
    "    print(f\"   üí∏ Est. cost for 100 questions: ${est_cost:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Configuration system ready!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# COMPREHENSIVE MODEL TESTING - All OpenAI Models\n",
    "# ====================================================================\n",
    "\n",
    "async def classify_with_forced_selection(question: str, available_topics: List[str], model_config: dict) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced classification that FORCES the model to choose from available topics.\n",
    "    No 'Other' option - must pick the best matching topic.\n",
    "    \"\"\"\n",
    "    # Create numbered topic list for clarity\n",
    "    numbered_topics = []\n",
    "    for i, topic in enumerate(available_topics, 1):\n",
    "        numbered_topics.append(f\"{i}. {topic}\")\n",
    "    \n",
    "    topics_str = \"\\n\".join(numbered_topics)\n",
    "    \n",
    "    # Enhanced prompt with clearer instructions\n",
    "    prompt = f\"\"\"CLASSIFICATION TASK: You must select exactly ONE topic from the list below that best matches the student question.\n",
    "\n",
    "AVAILABLE TOPICS (select one):\n",
    "{topics_str}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- You MUST choose one of the numbered topics above\n",
    "- Respond with ONLY the exact topic name (without the number)\n",
    "- Do NOT say \"Other\", \"None\", or refuse to answer\n",
    "- If uncertain, pick the closest related topic\n",
    "- Look for key concepts: registration, courses, portal access, scholarships, certificates\n",
    "\n",
    "STUDENT QUESTION: \"{question}\"\n",
    "\n",
    "RESPONSE (topic name only):\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Build API call with model-specific parameters\n",
    "        api_params = {\n",
    "            \"model\": model_config[\"model\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a classification expert. Always select from the provided topics. Never refuse or say 'Other'.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Add model-specific parameters\n",
    "        if model_config[\"max_tokens_param\"] == \"max_completion_tokens\":\n",
    "            api_params[\"max_completion_tokens\"] = model_config[\"max_tokens_value\"]\n",
    "        else:\n",
    "            api_params[\"max_tokens\"] = model_config[\"max_tokens_value\"]\n",
    "            \n",
    "        if model_config[\"temperature\"] is not None:\n",
    "            api_params[\"temperature\"] = model_config[\"temperature\"]\n",
    "            \n",
    "        response = await async_client.chat.completions.create(**api_params)\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Optional debug logging (comment out for production)\n",
    "        # print(f\"üîç Debug: '{question[:30]}...' -> Raw: '{result}' (len={len(result)})\")\n",
    "        \n",
    "        # Clean up response more thoroughly\n",
    "        result_clean = result.replace(\"‚Ä¢\", \"\").replace(\"-\", \"\").strip()\n",
    "        result_clean = result_clean.rstrip('.').rstrip(':').rstrip(',')\n",
    "        \n",
    "        # Remove quotes and extra whitespace\n",
    "        result_clean = result_clean.strip('\"').strip(\"'\").strip()\n",
    "        \n",
    "        # Remove any leading numbers (e.g., \"1. Topic\" -> \"Topic\")\n",
    "        import re\n",
    "        result_clean = re.sub(r'^\\d+\\.\\s*', '', result_clean)\n",
    "        result_clean = re.sub(r'^\\d+\\)\\s*', '', result_clean)  # Handle \"1) Topic\" format\n",
    "        \n",
    "        # Handle empty responses immediately\n",
    "        if not result_clean or len(result_clean) < 3:\n",
    "            print(f\"‚ö†Ô∏è  Empty response for: '{question[:50]}...'\")\n",
    "            return available_topics[0]\n",
    "        \n",
    "        # Direct exact match first (case-insensitive)\n",
    "        for topic in available_topics:\n",
    "            if result_clean.lower() == topic.lower():\n",
    "                return topic\n",
    "        \n",
    "        # Enhanced fuzzy matching with multiple strategies\n",
    "        best_match = None\n",
    "        max_score = 0\n",
    "        \n",
    "        for topic in available_topics:\n",
    "            topic_lower = topic.lower()\n",
    "            result_lower = result_clean.lower()\n",
    "            \n",
    "            # Strategy 1: Substring matching (both directions)\n",
    "            if topic_lower in result_lower or result_lower in topic_lower:\n",
    "                if len(result_clean) > 5:  # Avoid matching very short strings\n",
    "                    return topic\n",
    "            \n",
    "            # Strategy 2: Word-by-word matching\n",
    "            topic_words = set(topic_lower.split())\n",
    "            result_words = set(result_lower.split())\n",
    "            common_words = topic_words.intersection(result_words)\n",
    "            \n",
    "            if common_words:\n",
    "                score = len(common_words) / max(len(topic_words), len(result_words))\n",
    "                if score > max_score and score > 0.3:  # At least 30% word overlap\n",
    "                    max_score = score\n",
    "                    best_match = topic\n",
    "        \n",
    "        if best_match:\n",
    "            return best_match\n",
    "                \n",
    "        # Enhanced keyword-based fallback\n",
    "        question_lower = question.lower()\n",
    "        result_lower = result_clean.lower()\n",
    "        \n",
    "        # Extended keyword matching\n",
    "        keyword_groups = {\n",
    "            \"registration\": [\"registration\", \"register\", \"enroll\", \"signup\"],\n",
    "            \"course\": [\"course\", \"class\", \"lesson\", \"curriculum\"],\n",
    "            \"portal\": [\"portal\", \"website\", \"login\", \"access\", \"dashboard\"],\n",
    "            \"scholarship\": [\"scholarship\", \"financial\", \"aid\", \"funding\", \"money\"],\n",
    "            \"certificate\": [\"certificate\", \"diploma\", \"credential\", \"completion\"],\n",
    "            \"technical\": [\"technical\", \"tech\", \"computer\", \"software\", \"online\"],\n",
    "            \"pathway\": [\"pathway\", \"program\", \"byu\"]\n",
    "        }\n",
    "        \n",
    "        # Score topics based on keyword relevance\n",
    "        topic_scores = {}\n",
    "        for topic in available_topics:\n",
    "            topic_lower = topic.lower()\n",
    "            score = 0\n",
    "            \n",
    "            # Check for keyword matches\n",
    "            for category, keywords in keyword_groups.items():\n",
    "                for keyword in keywords:\n",
    "                    if keyword in question_lower and keyword in topic_lower:\n",
    "                        score += 2  # Strong match\n",
    "                    elif keyword in question_lower or keyword in result_lower:\n",
    "                        if keyword in topic_lower:\n",
    "                            score += 1  # Partial match\n",
    "            \n",
    "            if score > 0:\n",
    "                topic_scores[topic] = score\n",
    "        \n",
    "        # Return highest scoring topic if any keywords matched\n",
    "        if topic_scores:\n",
    "            best_topic = max(topic_scores.keys(), key=lambda t: topic_scores[t])\n",
    "            print(f\"‚ö° Keyword fallback: '{question[:30]}...' -> '{best_topic}' (score: {topic_scores[best_topic]})\")\n",
    "            return best_topic\n",
    "        \n",
    "        # Final fallback with better logging\n",
    "        print(f\"‚ö†Ô∏è  Final fallback used for: '{question[:50]}...'\")\n",
    "        print(f\"   Raw response: '{result}'\")\n",
    "        print(f\"   Cleaned: '{result_clean}'\")\n",
    "        print(f\"   -> Defaulting to: '{available_topics[0]}'\")\n",
    "        return available_topics[0]\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"‚ùå API Error classifying question: {error_msg}\")\n",
    "        print(f\"   Question: '{question[:50]}...'\")\n",
    "        print(f\"   Model: {model_config.get('model', 'unknown')}\")\n",
    "        \n",
    "        # Check for specific API errors\n",
    "        if \"max_completion_tokens\" in error_msg and model_config[\"max_tokens_param\"] == \"max_completion_tokens\":\n",
    "            print(f\"   üîß Hint: GPT-5 model parameter issue detected\")\n",
    "        elif \"max_tokens\" in error_msg and model_config[\"max_tokens_param\"] == \"max_tokens\":\n",
    "            print(f\"   üîß Hint: GPT-4 model parameter issue detected\")\n",
    "        elif \"temperature\" in error_msg:\n",
    "            print(f\"   üîß Hint: Temperature parameter issue detected\")\n",
    "        \n",
    "        return available_topics[0]  # Return first topic instead of \"Other\"\n",
    "\n",
    "async def test_all_models_comprehensive():\n",
    "    \"\"\"Test all available OpenAI models with comprehensive comparison\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ COMPREHENSIVE MODEL TESTING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìä Testing {MODEL_COMPARISON_SAMPLE_SIZE} questions across all models\")\n",
    "    print(f\"üéØ Using forced selection (no 'Other' allowed)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get sample data\n",
    "    test_sample = df_questions_topics.head(MODEL_COMPARISON_SAMPLE_SIZE).to_dict('records')\n",
    "    \n",
    "    # Models to test (prioritized order)\n",
    "    priority_models = [\n",
    "        \"gpt-5-nano\",      # Best for classification\n",
    "        \"gpt-5-mini\",      # Fast GPT-5\n",
    "        \"gpt-5\",           # Full GPT-5\n",
    "        \"gpt-4o-mini\",     # Current best baseline\n",
    "        \"gpt-4o\",          # Current flagship\n",
    "        \"gpt-4-turbo\",     # Previous generation\n",
    "        \"gpt-3.5-turbo\",   # Legacy but fast\n",
    "    ]\n",
    "    \n",
    "    model_results = {}\n",
    "    \n",
    "    for model_name in priority_models:\n",
    "        model_config = get_model_config(model_name)\n",
    "        print(f\"\\nüî¨ Testing {model_config['name']} ({model_name})\")\n",
    "        print(f\"   üí∞ Cost: ${model_config['pricing']['input']:.2f}/${model_config['pricing']['output']:.2f} per 1M tokens\")\n",
    "        \n",
    "        # Test model availability first\n",
    "        try:\n",
    "            test_params = {\n",
    "                \"model\": model_name,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "            }\n",
    "            if model_config[\"temperature\"] is not None:\n",
    "                test_params[\"temperature\"] = model_config[\"temperature\"]\n",
    "                \n",
    "            test_response = await async_client.chat.completions.create(**test_params)\n",
    "            print(f\"   ‚úÖ Model available\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Model unavailable: {str(e)[:100]}...\")\n",
    "            model_results[model_name] = {\"error\": str(e), \"available\": False}\n",
    "            continue\n",
    "        \n",
    "        # Run classification test\n",
    "        start_time = time.time()\n",
    "        sem = asyncio.Semaphore(8)  # Conservative concurrency for testing\n",
    "        \n",
    "        async def test_worker(question_data):\n",
    "            async with sem:\n",
    "                question = question_data['question']\n",
    "                correct_topic = question_data['cluster_topic']\n",
    "                \n",
    "                predicted_topic = await classify_with_forced_selection(question, topics_list, model_config)\n",
    "                \n",
    "                return {\n",
    "                    'question': question,\n",
    "                    'correct_topic': correct_topic,\n",
    "                    'predicted_topic': predicted_topic,\n",
    "                    'is_correct': predicted_topic == correct_topic\n",
    "                }\n",
    "        \n",
    "        try:\n",
    "            tasks = [test_worker(q_data) for q_data in test_sample]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            correct = sum(1 for r in results if r['is_correct'])\n",
    "            accuracy = (correct / len(results)) * 100\n",
    "            processing_time = end_time - start_time\n",
    "            \n",
    "            # Estimate cost\n",
    "            avg_input_tokens = 400  # Estimated tokens per prompt\n",
    "            avg_output_tokens = 20  # Estimated tokens per response\n",
    "            total_input = avg_input_tokens * len(results)\n",
    "            total_output = avg_output_tokens * len(results)\n",
    "            estimated_cost = estimate_cost(total_input, total_output, model_name)\n",
    "            \n",
    "            model_results[model_name] = {\n",
    "                \"available\": True,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"correct\": correct,\n",
    "                \"total\": len(results),\n",
    "                \"processing_time\": processing_time,\n",
    "                \"estimated_cost\": estimated_cost,\n",
    "                \"config\": model_config\n",
    "            }\n",
    "            \n",
    "            print(f\"   üéØ Accuracy: {accuracy:.1f}% ({correct}/{len(results)})\")\n",
    "            print(f\"   ‚ö° Time: {processing_time:.1f}s\")\n",
    "            print(f\"   üí∏ Est. cost: ${estimated_cost:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Testing failed: {str(e)[:100]}...\")\n",
    "            model_results[model_name] = {\"error\": str(e), \"available\": True, \"test_failed\": True}\n",
    "    \n",
    "    return model_results\n",
    "\n",
    "# Run comprehensive testing\n",
    "comprehensive_results = await test_all_models_comprehensive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# RESULTS ANALYSIS AND BEST MODEL SELECTION\n",
    "# ====================================================================\n",
    "\n",
    "def analyze_comprehensive_results(results: dict):\n",
    "    \"\"\"Analyze and display comprehensive testing results\"\"\"\n",
    "    \n",
    "    print(f\"\\nüèÜ COMPREHENSIVE MODEL COMPARISON RESULTS\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # Separate available and unavailable models\n",
    "    available_models = {k: v for k, v in results.items() if v.get(\"available\", False) and not v.get(\"test_failed\", False)}\n",
    "    unavailable_models = {k: v for k, v in results.items() if not v.get(\"available\", False) or v.get(\"test_failed\", False)}\n",
    "    \n",
    "    if available_models:\n",
    "        print(f\"‚úÖ SUCCESSFUL TESTS ({len(available_models)} models):\")\n",
    "        print(\"-\" * 90)\n",
    "        print(f\"{'Model':<25} {'Accuracy':<12} {'Time':<8} {'Cost':<12} {'Notes':<20}\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        # Sort by accuracy (best first)\n",
    "        sorted_models = sorted(available_models.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "        \n",
    "        best_model = None\n",
    "        best_accuracy = 0\n",
    "        \n",
    "        for model_name, result in sorted_models:\n",
    "            accuracy = result['accuracy']\n",
    "            time_taken = result['processing_time']\n",
    "            cost = result['estimated_cost']\n",
    "            \n",
    "            # Determine best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model_name\n",
    "            \n",
    "            # Performance indicators\n",
    "            if accuracy >= 80:\n",
    "                perf_indicator = \"üî• EXCELLENT\"\n",
    "            elif accuracy >= 60:\n",
    "                perf_indicator = \"üåü VERY GOOD\"\n",
    "            elif accuracy >= 40:\n",
    "                perf_indicator = \"‚ö° GOOD\"\n",
    "            elif accuracy >= 20:\n",
    "                perf_indicator = \"üìà FAIR\"\n",
    "            else:\n",
    "                perf_indicator = \"üîÑ NEEDS WORK\"\n",
    "            \n",
    "            print(f\"{model_name:<25} {accuracy:>6.1f}% {time_taken:>6.1f}s ${cost:>8.6f} {perf_indicator}\")\n",
    "        \n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        if best_model:\n",
    "            best_result = available_models[best_model]\n",
    "            best_config = best_result['config']\n",
    "            \n",
    "            print(f\"\\nüèÜ BEST PERFORMING MODEL: {best_config['name']}\")\n",
    "            print(f\"   üìä Model ID: {best_model}\")\n",
    "            print(f\"   üéØ Accuracy: {best_accuracy:.1f}% ({best_result['correct']}/{best_result['total']})\")\n",
    "            print(f\"   ‚ö° Processing time: {best_result['processing_time']:.1f} seconds\")\n",
    "            print(f\"   üí∞ Estimated cost: ${best_result['estimated_cost']:.6f}\")\n",
    "            print(f\"   üí∏ Pricing: ${best_config['pricing']['input']:.2f}/${best_config['pricing']['output']:.2f} per 1M tokens\")\n",
    "            \n",
    "            # Cost projection for larger tests\n",
    "            cost_100 = estimate_cost(40000, 2000, best_model)  # ~400 tokens per question\n",
    "            cost_full = estimate_cost(400000 * len(df_questions_topics) // 100, 20000 * len(df_questions_topics) // 100, best_model)\n",
    "            \n",
    "            print(f\"\\nüìà COST PROJECTIONS:\")\n",
    "            print(f\"   üíµ {BEST_MODEL_TEST_SIZE} questions: ~${cost_100:.4f}\")\n",
    "            print(f\"   üíµ All {len(df_questions_topics)} questions: ~${cost_full:.2f}\")\n",
    "            \n",
    "            # Set global best model variable\n",
    "            global GLOBAL_BEST_MODEL, GLOBAL_BEST_CONFIG\n",
    "            GLOBAL_BEST_MODEL = best_model\n",
    "            GLOBAL_BEST_CONFIG = best_config\n",
    "            \n",
    "            print(f\"\\n‚úÖ Best model saved as: GLOBAL_BEST_MODEL = '{best_model}'\")\n",
    "    \n",
    "    if unavailable_models:\n",
    "        print(f\"\\n‚ùå UNAVAILABLE/FAILED MODELS ({len(unavailable_models)} models):\")\n",
    "        print(\"-\" * 60)\n",
    "        for model_name, result in unavailable_models.items():\n",
    "            error_msg = result.get('error', 'Unknown error')[:50]\n",
    "            print(f\"   {model_name:<25} {error_msg}\")\n",
    "    \n",
    "    print(\"=\"*90)\n",
    "    return best_model if available_models else None\n",
    "\n",
    "# Analyze results and select best model\n",
    "selected_best_model = analyze_comprehensive_results(comprehensive_results)\n",
    "\n",
    "if selected_best_model:\n",
    "    print(f\"\\nüéâ READY FOR DETAILED TESTING WITH: {GLOBAL_BEST_CONFIG['name']}\")\n",
    "    print(f\"   Next step: Run detailed test with {BEST_MODEL_TEST_SIZE} questions\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No models available for testing. Check your API access.\")\n",
    "    # Fallback to a known working model\n",
    "    GLOBAL_BEST_MODEL = \"gpt-4o-mini\"\n",
    "    GLOBAL_BEST_CONFIG = get_model_config(GLOBAL_BEST_MODEL)\n",
    "    print(f\"   Falling back to: {GLOBAL_BEST_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# DETAILED TESTING WITH BEST MODEL\n",
    "# ====================================================================\n",
    "\n",
    "async def run_detailed_test_with_best_model():\n",
    "    \"\"\"Run detailed test with the best performing model\"\"\"\n",
    "    \n",
    "    if 'GLOBAL_BEST_MODEL' not in globals():\n",
    "        print(\"‚ùå No best model selected. Please run the comprehensive test first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üöÄ DETAILED TESTING WITH BEST MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìã Model: {GLOBAL_BEST_CONFIG['name']} ({GLOBAL_BEST_MODEL})\")\n",
    "    print(f\"üìä Sample size: {BEST_MODEL_TEST_SIZE} questions\")\n",
    "    print(f\"üéØ Using enhanced forced-selection prompt\")\n",
    "    print(f\"üí∞ Pricing: ${GLOBAL_BEST_CONFIG['pricing']['input']:.2f}/${GLOBAL_BEST_CONFIG['pricing']['output']:.2f} per 1M tokens\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get test sample\n",
    "    test_data = df_questions_topics.head(BEST_MODEL_TEST_SIZE).to_dict('records')\n",
    "    \n",
    "    # Run classification\n",
    "    start_time = time.time()\n",
    "    sem = asyncio.Semaphore(OPTIMAL_CONCURRENCY)\n",
    "    \n",
    "    async def detailed_worker(question_data):\n",
    "        async with sem:\n",
    "            question = question_data['question']\n",
    "            correct_topic = question_data['cluster_topic']\n",
    "            \n",
    "            predicted_topic = await classify_with_forced_selection(question, topics_list, GLOBAL_BEST_CONFIG)\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'correct_topic': correct_topic,\n",
    "                'predicted_topic': predicted_topic,\n",
    "                'is_correct': predicted_topic == correct_topic\n",
    "            }\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(test_data)} questions...\")\n",
    "    tasks = [detailed_worker(q_data) for q_data in test_data]\n",
    "    detailed_results = await asyncio.gather(*tasks)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    total_questions = len(detailed_results)\n",
    "    correct_count = sum(1 for r in detailed_results if r['is_correct'])\n",
    "    accuracy = (correct_count / total_questions) * 100\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Estimate actual cost\n",
    "    avg_input_tokens = 450  # More accurate estimate for detailed prompt\n",
    "    avg_output_tokens = 25  # Response tokens\n",
    "    total_input = avg_input_tokens * total_questions\n",
    "    total_output = avg_output_tokens * total_questions\n",
    "    actual_cost = estimate_cost(total_input, total_output, GLOBAL_BEST_MODEL)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"üèÜ DETAILED TEST RESULTS - {GLOBAL_BEST_CONFIG['name']}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìä Total questions: {total_questions}\")\n",
    "    print(f\"‚úÖ Correct predictions: {correct_count}\")\n",
    "    print(f\"‚ùå Incorrect predictions: {total_questions - correct_count}\")\n",
    "    print(f\"üéØ Accuracy: {accuracy:.1f}%\")\n",
    "    print(f\"‚ö° Processing time: {processing_time:.1f} seconds\")\n",
    "    print(f\"üìà Questions per second: {total_questions / processing_time:.1f}\")\n",
    "    print(f\"üí∏ Actual cost: ${actual_cost:.4f}\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    if accuracy >= 90:\n",
    "        print(f\"\\nüéâ EXCELLENT! {GLOBAL_BEST_CONFIG['name']} achieved {accuracy:.1f}% accuracy (‚â•90%)\")\n",
    "        print(\"‚úÖ RECOMMENDATION: Use OpenAI classification (skip clustering)\")\n",
    "        recommendation = \"USE_OPENAI\"\n",
    "    elif accuracy >= 75:\n",
    "        print(f\"\\nüåü VERY GOOD! {GLOBAL_BEST_CONFIG['name']} achieved {accuracy:.1f}% accuracy\")\n",
    "        print(\"‚ö° RECOMMENDATION: Consider hybrid approach or proceed with OpenAI\")\n",
    "        recommendation = \"HYBRID_OR_OPENAI\"\n",
    "    elif accuracy >= 60:\n",
    "        print(f\"\\nüìà GOOD PROGRESS! {GLOBAL_BEST_CONFIG['name']} achieved {accuracy:.1f}% accuracy\")\n",
    "        print(\"üîß RECOMMENDATION: Optimize prompts or use hybrid approach\")\n",
    "        recommendation = \"OPTIMIZE_OR_HYBRID\"\n",
    "    else:\n",
    "        print(f\"\\nüîÑ NEEDS IMPROVEMENT: {GLOBAL_BEST_CONFIG['name']} achieved {accuracy:.1f}% accuracy\")\n",
    "        print(\"üõ†Ô∏è  RECOMMENDATION: Stick with clustering approach\")\n",
    "        recommendation = \"USE_CLUSTERING\"\n",
    "    \n",
    "    # Cost projections for full dataset\n",
    "    if RUN_FULL_DATASET_TEST:\n",
    "        full_cost = estimate_cost(\n",
    "            avg_input_tokens * len(df_questions_topics),\n",
    "            avg_output_tokens * len(df_questions_topics),\n",
    "            GLOBAL_BEST_MODEL\n",
    "        )\n",
    "        print(f\"\\nüí∞ FULL DATASET PROJECTION:\")\n",
    "        print(f\"   üìä Total questions: {len(df_questions_topics)}\")\n",
    "        print(f\"   üí∏ Estimated cost: ${full_cost:.2f}\")\n",
    "        print(f\"   ‚è±Ô∏è Estimated time: {(len(df_questions_topics) / (total_questions / processing_time)):.0f} seconds\")\n",
    "    \n",
    "    # Error analysis (show a few misclassifications)\n",
    "    incorrect_results = [r for r in detailed_results if not r['is_correct']]\n",
    "    if incorrect_results:\n",
    "        print(f\"\\nüîç SAMPLE MISCLASSIFICATIONS (showing 5 of {len(incorrect_results)}):\")\n",
    "        print(\"-\" * 70)\n",
    "        for i, result in enumerate(incorrect_results[:5], 1):\n",
    "            print(f\"{i}. Question: {result['question'][:60]}...\")\n",
    "            print(f\"   Expected: {result['correct_topic']}\")\n",
    "            print(f\"   Predicted: {result['predicted_topic']}\")\n",
    "            print()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Store results for potential full dataset test\n",
    "    global DETAILED_TEST_RESULTS\n",
    "    DETAILED_TEST_RESULTS = {\n",
    "        'accuracy': accuracy,\n",
    "        'recommendation': recommendation,\n",
    "        'cost_per_question': actual_cost / total_questions,\n",
    "        'processing_time': processing_time\n",
    "    }\n",
    "    \n",
    "    return detailed_results\n",
    "\n",
    "# Run detailed test\n",
    "detailed_test_results = await run_detailed_test_with_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# FULL DATASET TESTING (Conditional)\n",
    "# ====================================================================\n",
    "\n",
    "async def run_full_dataset_test():\n",
    "    \"\"\"Run test on all questions if enabled\"\"\"\n",
    "    \n",
    "    if not RUN_FULL_DATASET_TEST:\n",
    "        print(\"‚ÑπÔ∏è  FULL DATASET TEST DISABLED\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"To enable full dataset testing:\")\n",
    "        print(\"1. Set RUN_FULL_DATASET_TEST = True in the configuration\")\n",
    "        print(\"2. Re-run this cell\")\n",
    "        print(f\"\\nCurrent settings:\")\n",
    "        print(f\"   üîß RUN_FULL_DATASET_TEST = {RUN_FULL_DATASET_TEST}\")\n",
    "        print(f\"   üìä Total questions available: {len(df_questions_topics)}\")\n",
    "        \n",
    "        if 'DETAILED_TEST_RESULTS' in globals():\n",
    "            projected_cost = DETAILED_TEST_RESULTS['cost_per_question'] * len(df_questions_topics)\n",
    "            projected_time = (len(df_questions_topics) / 100) * DETAILED_TEST_RESULTS['processing_time']\n",
    "            print(f\"   üí∏ Projected cost: ${projected_cost:.2f}\")\n",
    "            print(f\"   ‚è±Ô∏è Projected time: {projected_time:.0f} seconds ({projected_time/60:.1f} minutes)\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        return None\n",
    "    \n",
    "    if 'GLOBAL_BEST_MODEL' not in globals():\n",
    "        print(\"‚ùå No best model selected. Please run the comprehensive test first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üöÄ FULL DATASET TESTING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìã Model: {GLOBAL_BEST_CONFIG['name']} ({GLOBAL_BEST_MODEL})\")\n",
    "    print(f\"üìä Total questions: {len(df_questions_topics)}\")\n",
    "    print(f\"üéØ Using optimized forced-selection approach\")\n",
    "    print(f\"‚ö° Concurrency: {OPTIMAL_CONCURRENCY} parallel requests\")\n",
    "    \n",
    "    # Cost and time estimates\n",
    "    if 'DETAILED_TEST_RESULTS' in globals():\n",
    "        projected_cost = DETAILED_TEST_RESULTS['cost_per_question'] * len(df_questions_topics)\n",
    "        projected_time = (len(df_questions_topics) / 100) * DETAILED_TEST_RESULTS['processing_time']\n",
    "        print(f\"üí∏ Estimated cost: ${projected_cost:.2f}\")\n",
    "        print(f\"‚è±Ô∏è Estimated time: {projected_time:.0f} seconds ({projected_time/60:.1f} minutes)\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Confirm with user (in a real scenario)\n",
    "    print(\"‚ö†Ô∏è  WARNING: This will test ALL questions and incur API costs!\")\n",
    "    print(\"üîÑ Starting full dataset test...\")\n",
    "    \n",
    "    # Get all question data\n",
    "    all_question_data = df_questions_topics.to_dict('records')\n",
    "    \n",
    "    # Run classification with progress tracking\n",
    "    start_time = time.time()\n",
    "    sem = asyncio.Semaphore(OPTIMAL_CONCURRENCY)\n",
    "    \n",
    "    # Progress tracking\n",
    "    completed = 0\n",
    "    total = len(all_question_data)\n",
    "    \n",
    "    async def full_dataset_worker(question_data):\n",
    "        nonlocal completed\n",
    "        async with sem:\n",
    "            question = question_data['question']\n",
    "            correct_topic = question_data['cluster_topic']\n",
    "            \n",
    "            predicted_topic = await classify_with_forced_selection(question, topics_list, GLOBAL_BEST_CONFIG)\n",
    "            \n",
    "            completed += 1\n",
    "            if completed % 100 == 0:  # Progress every 100 questions\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = completed / elapsed\n",
    "                remaining = (total - completed) / rate\n",
    "                print(f\"   üìà Progress: {completed}/{total} ({100*completed/total:.1f}%) - ETA: {remaining:.0f}s\")\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'correct_topic': correct_topic,\n",
    "                'predicted_topic': predicted_topic,\n",
    "                'is_correct': predicted_topic == correct_topic\n",
    "            }\n",
    "    \n",
    "    print(f\"üîÑ Processing {total} questions...\")\n",
    "    tasks = [full_dataset_worker(q_data) for q_data in all_question_data]\n",
    "    full_results = await asyncio.gather(*tasks)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    total_questions = len(full_results)\n",
    "    correct_count = sum(1 for r in full_results if r['is_correct'])\n",
    "    final_accuracy = (correct_count / total_questions) * 100\n",
    "    total_processing_time = end_time - start_time\n",
    "    \n",
    "    # Calculate actual cost\n",
    "    avg_input_tokens = 450\n",
    "    avg_output_tokens = 25\n",
    "    total_input = avg_input_tokens * total_questions\n",
    "    total_output = avg_output_tokens * total_questions\n",
    "    total_cost = estimate_cost(total_input, total_output, GLOBAL_BEST_MODEL)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"üèÜ FINAL RESULTS - FULL DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìä Total questions processed: {total_questions}\")\n",
    "    print(f\"‚úÖ Correct predictions: {correct_count}\")\n",
    "    print(f\"‚ùå Incorrect predictions: {total_questions - correct_count}\")\n",
    "    print(f\"üéØ Final accuracy: {final_accuracy:.1f}%\")\n",
    "    print(f\"‚ö° Total processing time: {total_processing_time:.1f} seconds ({total_processing_time/60:.1f} minutes)\")\n",
    "    print(f\"üìà Questions per second: {total_questions / total_processing_time:.1f}\")\n",
    "    print(f\"üí∏ Total cost: ${total_cost:.2f}\")\n",
    "    \n",
    "    # Final recommendation\n",
    "    print(f\"\\nüèÜ FINAL DECISION:\")\n",
    "    if final_accuracy >= 90:\n",
    "        print(f\"üéâ EXCELLENT! {final_accuracy:.1f}% accuracy achieved!\")\n",
    "        print(\"‚úÖ STRONG RECOMMENDATION: Replace clustering with OpenAI classification\")\n",
    "        print(\"üí° Benefits: Faster processing, no training needed, consistent results\")\n",
    "    elif final_accuracy >= 75:\n",
    "        print(f\"üåü VERY GOOD! {final_accuracy:.1f}% accuracy achieved!\")\n",
    "        print(\"‚ö° RECOMMENDATION: Consider OpenAI classification or hybrid approach\")\n",
    "        print(\"üí° OpenAI classification is viable for this use case\")\n",
    "    elif final_accuracy >= 60:\n",
    "        print(f\"üìà MODERATE SUCCESS: {final_accuracy:.1f}% accuracy achieved\")\n",
    "        print(\"üîß RECOMMENDATION: Optimize prompts or use hybrid approach\")\n",
    "        print(\"üí° Clustering + OpenAI fallback might be optimal\")\n",
    "    else:\n",
    "        print(f\"üîÑ BELOW TARGET: {final_accuracy:.1f}% accuracy achieved\")\n",
    "        print(\"üõ†Ô∏è  RECOMMENDATION: Continue with clustering approach\")\n",
    "        print(\"üí° OpenAI not yet suitable for replacing clustering\")\n",
    "    \n",
    "    # Comparison with clustering baseline\n",
    "    print(f\"\\nüìä PERFORMANCE COMPARISON:\")\n",
    "    print(f\"   ü§ñ OpenAI ({GLOBAL_BEST_CONFIG['name']}): {final_accuracy:.1f}%\")\n",
    "    print(f\"   üß© Clustering baseline: ~85-95% (estimated)\")\n",
    "    print(f\"   üìà Gap: {final_accuracy - 90:.1f} percentage points from target\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return full_results\n",
    "\n",
    "# Run full dataset test (conditional)\n",
    "full_dataset_results = await run_full_dataset_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Testing with Enhanced Prompt\n",
    "\n",
    "This section implements the most advanced classification approach with enhanced prompts, keyword fallback strategies, and optimal settings to achieve the highest possible accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def classify_question_comprehensive(question: str, available_topics: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Comprehensive classification approach with enhanced prompt and fallback strategies\n",
    "    \"\"\"\n",
    "    topics_str = \"\\n\".join([f\"‚Ä¢ {topic}\" for topic in available_topics])\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert at classifying BYU-Pathway student questions into specific topics.\n",
    "\n",
    "AVAILABLE TOPICS:\n",
    "{topics_str}\n",
    "‚Ä¢ Other (only if nothing else fits)\n",
    "\n",
    "CLASSIFICATION RULES:\n",
    "1. Look for keywords and context in the question\n",
    "2. Choose the MOST SPECIFIC topic that matches\n",
    "3. Key patterns:\n",
    "   - \"religion courses\" ‚Üí Religion Course Registration and Requirements\n",
    "   - \"certificate\" or \"completion\" ‚Üí Certificate Application Process  \n",
    "   - \"scholarship\" or \"Heber\" ‚Üí Heber J. Grant Scholarship Application Process\n",
    "   - \"BYU Pathway\" or \"accredited\" ‚Üí BYU-Pathway Program\n",
    "   - \"English Connect 3\" or \"EC3\" ‚Üí English Connect 3 Registration Process\n",
    "   - \"registration\" ‚Üí match to most specific registration topic\n",
    "   - \"portal\" or \"login\" ‚Üí appropriate Student Portal topic\n",
    "4. Avoid \"Other\" unless truly nothing fits\n",
    "\n",
    "QUESTION: \"{question}\"\n",
    "\n",
    "Respond with ONLY the exact topic name from the list above:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = await async_client.chat.completions.create(\n",
    "            model=OPTIMAL_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert classifier. Choose the most specific matching topic. Avoid 'Other' when possible.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean up response and find exact match\n",
    "        result_clean = result.replace(\"‚Ä¢\", \"\").replace(\"-\", \"\").strip().rstrip('.')\n",
    "        \n",
    "        # Direct match\n",
    "        if result_clean in available_topics:\n",
    "            return result_clean\n",
    "        \n",
    "        # Fuzzy matching for partial responses\n",
    "        for topic in available_topics:\n",
    "            if topic.lower() in result_clean.lower() or result_clean.lower() in topic.lower():\n",
    "                if len(result_clean) > 5:  # Avoid very short matches\n",
    "                    return topic\n",
    "        \n",
    "        # Keyword fallback strategies\n",
    "        question_lower = question.lower()\n",
    "        if \"religion\" in question_lower and \"course\" in question_lower:\n",
    "            return \"Religion Course Registration and Requirements\"\n",
    "        elif \"scholarship\" in question_lower or \"heber\" in question_lower:\n",
    "            return \"Heber J. Grant Scholarship Application Process\"\n",
    "        elif (\"byu pathway\" in question_lower or \"accredited\" in question_lower):\n",
    "            return \"BYU-Pathway Program\"\n",
    "        elif (\"english connect 3\" in question_lower or \"ec3\" in question_lower):\n",
    "            return \"English Connect 3 Registration Process\"\n",
    "        elif \"certificate\" in question_lower and (\"completion\" in question_lower or \"complete\" in question_lower):\n",
    "            return \"Certificate Application Process\"\n",
    "        \n",
    "        return \"Other\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error classifying question: {e}\")\n",
    "        return \"Other\"\n",
    "\n",
    "async def worker_comprehensive(question_data: Dict, sem: asyncio.Semaphore, available_topics: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive worker with optimal settings and error handling\n",
    "    \"\"\"\n",
    "    async with sem:\n",
    "        question = question_data['question']\n",
    "        correct_topic = question_data['cluster_topic']\n",
    "        \n",
    "        openai_topic = await classify_question_comprehensive(question, available_topics)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'correct_topic': correct_topic,\n",
    "            'openai_topic': openai_topic,\n",
    "            'is_correct': openai_topic == correct_topic\n",
    "        }\n",
    "\n",
    "# Comprehensive Test on 100 Questions\n",
    "print(\"\\nüöÄ COMPREHENSIVE PARALLEL TESTING\")\n",
    "print(\"=\"*50)\n",
    "print(\"Testing with optimal settings and enhanced prompt on 100 questions...\")\n",
    "\n",
    "test_sample = df_questions_topics.head(100).to_dict('records')\n",
    "\n",
    "start_time = time.time()\n",
    "sem = asyncio.Semaphore(OPTIMAL_CONCURRENCY)\n",
    "tasks = [asyncio.create_task(worker_comprehensive(q_data, sem, topics_list)) for q_data in test_sample]\n",
    "comprehensive_results = await asyncio.gather(*tasks)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate comprehensive results\n",
    "comprehensive_correct = sum(1 for result in comprehensive_results if result['is_correct'])\n",
    "comprehensive_accuracy = (comprehensive_correct / len(comprehensive_results)) * 100\n",
    "\n",
    "print(f\"\\nüìä COMPREHENSIVE TEST RESULTS:\")\n",
    "print(f\"‚úÖ Accuracy: {comprehensive_accuracy:.1f}%\")\n",
    "print(f\"‚ö° Processing time: {end_time - start_time:.1f} seconds\")\n",
    "print(f\"üìà Questions tested: {len(comprehensive_results)}\")\n",
    "print(f\"‚úì Correct: {comprehensive_correct}\")\n",
    "print(f\"‚úó Incorrect: {len(comprehensive_results) - comprehensive_correct}\")\n",
    "\n",
    "# Determine next steps\n",
    "if comprehensive_accuracy >= 90:\n",
    "    print(f\"\\nüèÜ EXCELLENT! Achieved {comprehensive_accuracy:.1f}% accuracy (‚â•90%)\")\n",
    "    print(\"‚úÖ RECOMMENDATION: Proceed with OpenAI classification (skip clustering)\")\n",
    "elif comprehensive_accuracy >= 80:\n",
    "    print(f\"\\nüåü VERY GOOD! {comprehensive_accuracy:.1f}% accuracy (close to target)\")\n",
    "    print(\"‚ö° RECOMMENDATION: Consider hybrid approach or minor refinements\")\n",
    "elif comprehensive_accuracy >= 70:\n",
    "    print(f\"\\nüìà GOOD PROGRESS! {comprehensive_accuracy:.1f}% accuracy\")\n",
    "    print(\"üîß RECOMMENDATION: Continue refining prompt or use hybrid approach\")\n",
    "else:\n",
    "    print(f\"\\nüîÑ NEEDS MORE WORK: {comprehensive_accuracy:.1f}% accuracy\")\n",
    "    print(\"üõ†Ô∏è  RECOMMENDATION: Stick with clustering approach for now\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Different Models (Including Checking for Newer Models)\n",
    "print(\"üîç TESTING DIFFERENT OPENAI MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# List of models to test (based on confirmed availability)\n",
    "models_to_test = [\n",
    "    \"gpt-4o-mini\",           # Current best performer\n",
    "    \"gpt-4o\",                # Previous test model\n",
    "    \"gpt-5\",                 # ‚úÖ CONFIRMED: Released August 7, 2025\n",
    "    \"gpt-5-thinking\",        # GPT-5 with reasoning mode\n",
    "    \"gpt-5-mini\",            # Mini version mentioned in GPT-5 docs\n",
    "    \"gpt-5-nano\",            # ‚úÖ CONFIRMED: Fastest, cheapest GPT-5 for classification\n",
    "    \"gpt-4-turbo\",           # Alternative model\n",
    "    \"gpt-3.5-turbo\",         # Faster/cheaper option\n",
    "    \"gpt-4o-2024-08-06\",     # Specific version\n",
    "]\n",
    "\n",
    "print(\"üîç CONFIRMED MODEL INFORMATION:\")\n",
    "print(\"‚úÖ GPT-5: Released August 7, 2025 - 'Our smartest, fastest, most useful model yet'\")\n",
    "print(\"‚úÖ GPT-5 Thinking: Extended reasoning version\")\n",
    "print(\"‚úÖ GPT-5 Mini: Smaller, faster version mentioned in docs\")\n",
    "print(\"‚úÖ GPT-5 Nano: CONFIRMED - 'Fastest, cheapest GPT-5 for classification tasks'\")\n",
    "print(\"   üí∞ Pricing: $0.05 input / $0.40 output per 1M tokens\")\n",
    "print(\"   üéØ Perfect for classification tasks like ours!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Function to test if a model exists and works\n",
    "async def test_model_availability(model_name: str) -> bool:\n",
    "    \"\"\"Test if a model is available and working\"\"\"\n",
    "    try:\n",
    "        response = await async_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n",
    "            max_tokens=5\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {model_name}: {str(e)[:100]}...\")\n",
    "        return False\n",
    "\n",
    "# Test model availability\n",
    "print(\"Testing model availability...\")\n",
    "available_models = []\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"Testing {model}...\", end=\" \")\n",
    "    is_available = await test_model_availability(model)\n",
    "    if is_available:\n",
    "        print(\"‚úÖ Available\")\n",
    "        available_models.append(model)\n",
    "    else:\n",
    "        print(\"‚ùå Not available\")\n",
    "\n",
    "print(f\"\\n‚úÖ Available models: {available_models}\")\n",
    "\n",
    "# Test classification with available models on sample questions\n",
    "if available_models:\n",
    "    print(f\"\\nüß™ TESTING CLASSIFICATION ACCURACY WITH AVAILABLE MODELS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Use a smaller sample for testing multiple models\n",
    "    test_sample_small = df_questions_topics.head(20).to_dict('records')\n",
    "    model_results = {}\n",
    "    \n",
    "    for model in available_models:\n",
    "        print(f\"\\nüî¨ Testing {model}...\")\n",
    "        \n",
    "        # Modified classification function for different models\n",
    "        async def classify_with_specific_model(question: str, available_topics: List[str], model: str) -> str:\n",
    "            topics_str = \"\\n\".join([f\"‚Ä¢ {topic}\" for topic in available_topics])\n",
    "            \n",
    "            prompt = f\"\"\"You are an expert at classifying BYU-Pathway student questions into specific topics.\n",
    "\n",
    "AVAILABLE TOPICS:\n",
    "{topics_str}\n",
    "‚Ä¢ Other (only if nothing else fits)\n",
    "\n",
    "CLASSIFICATION RULES:\n",
    "1. Look for keywords and context in the question\n",
    "2. Choose the MOST SPECIFIC topic that matches\n",
    "3. Key patterns:\n",
    "   - \"religion courses\" ‚Üí Religion Course Registration and Requirements\n",
    "   - \"certificate\" or \"completion\" ‚Üí Certificate Application Process  \n",
    "   - \"scholarship\" or \"Heber\" ‚Üí Heber J. Grant Scholarship Application Process\n",
    "   - \"BYU Pathway\" or \"accredited\" ‚Üí BYU-Pathway Program\n",
    "   - \"English Connect 3\" or \"EC3\" ‚Üí English Connect 3 Registration Process\n",
    "4. Avoid \"Other\" unless truly nothing fits\n",
    "\n",
    "QUESTION: \"{question}\"\n",
    "\n",
    "Respond with ONLY the exact topic name from the list above:\"\"\"\n",
    "\n",
    "            try:\n",
    "                response = await async_client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert classifier. Choose the most specific matching topic. Avoid 'Other' when possible.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=100,\n",
    "                    temperature=0\n",
    "                )\n",
    "                \n",
    "                result = response.choices[0].message.content.strip()\n",
    "                result_clean = result.replace(\"‚Ä¢\", \"\").replace(\"-\", \"\").strip().rstrip('.')\n",
    "                \n",
    "                # Find exact match\n",
    "                if result_clean in available_topics:\n",
    "                    return result_clean\n",
    "                \n",
    "                # Fuzzy matching\n",
    "                for topic in available_topics:\n",
    "                    if topic.lower() in result_clean.lower() or result_clean.lower() in topic.lower():\n",
    "                        if len(result_clean) > 5:\n",
    "                            return topic\n",
    "                \n",
    "                return \"Other\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error with {model}: {e}\")\n",
    "                return \"Other\"\n",
    "\n",
    "        # Test this model\n",
    "        start_time = time.time()\n",
    "        sem = asyncio.Semaphore(8)  # Lower concurrency for testing\n",
    "        \n",
    "        tasks = []\n",
    "        for q_data in test_sample_small:\n",
    "            async def worker_for_model(question_data, model_name):\n",
    "                async with sem:\n",
    "                    question = question_data['question']\n",
    "                    correct_topic = question_data['cluster_topic']\n",
    "                    \n",
    "                    openai_topic = await classify_with_specific_model(question, topics_list, model_name)\n",
    "                    \n",
    "                    return {\n",
    "                        'question': question,\n",
    "                        'correct_topic': correct_topic,\n",
    "                        'openai_topic': openai_topic,\n",
    "                        'is_correct': openai_topic == correct_topic\n",
    "                    }\n",
    "            \n",
    "            tasks.append(asyncio.create_task(worker_for_model(q_data, model)))\n",
    "        \n",
    "        try:\n",
    "            results = await asyncio.gather(*tasks)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate results\n",
    "            correct = sum(1 for r in results if r['is_correct'])\n",
    "            accuracy = (correct / len(results)) * 100\n",
    "            processing_time = end_time - start_time\n",
    "            \n",
    "            model_results[model] = {\n",
    "                'accuracy': accuracy,\n",
    "                'correct': correct,\n",
    "                'total': len(results),\n",
    "                'time': processing_time\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ Accuracy: {accuracy:.1f}% ({correct}/{len(results)})\")\n",
    "            print(f\"   ‚ö° Time: {processing_time:.1f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed to test {model}: {e}\")\n",
    "            model_results[model] = {'accuracy': 0, 'error': str(e)}\n",
    "\n",
    "    # Summary of model comparison\n",
    "    print(f\"\\nüìä MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for model, results in model_results.items():\n",
    "        if 'accuracy' in results and 'error' not in results:\n",
    "            accuracy = results['accuracy']\n",
    "            time_taken = results['time']\n",
    "            print(f\"{model:20} | {accuracy:5.1f}% | {time_taken:4.1f}s\")\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "        else:\n",
    "            print(f\"{model:20} | ERROR\")\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"\\nüèÜ BEST PERFORMING MODEL: {best_model} ({best_accuracy:.1f}% accuracy)\")\n",
    "        print(f\"üí° Recommendation: Use {best_model} for full testing\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No models available for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Test with Best Model on All Questions\n",
    "if 'best_model' in locals() and best_model:\n",
    "    print(f\"\\nüöÄ FULL TEST WITH BEST MODEL: {best_model}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Testing on ALL {len(df_questions_topics)} questions...\")\n",
    "    \n",
    "    # Use the best model for comprehensive testing\n",
    "    BEST_MODEL = best_model\n",
    "    \n",
    "    async def classify_with_best_model(question: str, available_topics: List[str]) -> str:\n",
    "        \"\"\"Classification using the best performing model\"\"\"\n",
    "        topics_str = \"\\n\".join([f\"‚Ä¢ {topic}\" for topic in available_topics])\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert at classifying BYU-Pathway student questions into specific topics.\n",
    "\n",
    "AVAILABLE TOPICS:\n",
    "{topics_str}\n",
    "‚Ä¢ Other (only if nothing else fits)\n",
    "\n",
    "CLASSIFICATION RULES:\n",
    "1. Look for keywords and context in the question\n",
    "2. Choose the MOST SPECIFIC topic that matches\n",
    "3. Key patterns:\n",
    "   - \"religion courses\" ‚Üí Religion Course Registration and Requirements\n",
    "   - \"certificate\" or \"completion\" ‚Üí Certificate Application Process  \n",
    "   - \"scholarship\" or \"Heber\" ‚Üí Heber J. Grant Scholarship Application Process\n",
    "   - \"BYU Pathway\" or \"accredited\" ‚Üí BYU-Pathway Program\n",
    "   - \"English Connect 3\" or \"EC3\" ‚Üí English Connect 3 Registration Process\n",
    "   - \"registration\" ‚Üí match to most specific registration topic\n",
    "   - \"portal\" or \"login\" ‚Üí appropriate Student Portal topic\n",
    "4. Avoid \"Other\" unless truly nothing fits\n",
    "\n",
    "QUESTION: \"{question}\"\n",
    "\n",
    "Respond with ONLY the exact topic name from the list above:\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Use different parameters based on model type\n",
    "            if BEST_MODEL.startswith('gpt-5'):\n",
    "                # GPT-5 models use max_completion_tokens and don't support temperature=0\n",
    "                response = await async_client.chat.completions.create(\n",
    "                    model=BEST_MODEL,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert classifier. Choose the most specific matching topic. Avoid 'Other' when possible.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_completion_tokens=100\n",
    "                )\n",
    "            else:\n",
    "                # GPT-4 models use max_tokens and support temperature=0\n",
    "                response = await async_client.chat.completions.create(\n",
    "                    model=BEST_MODEL,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert classifier. Choose the most specific matching topic. Avoid 'Other' when possible.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=100,\n",
    "                    temperature=0\n",
    "                )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip()\n",
    "            result_clean = result.replace(\"‚Ä¢\", \"\").replace(\"-\", \"\").strip().rstrip('.')\n",
    "            \n",
    "            # Direct match\n",
    "            if result_clean in available_topics:\n",
    "                return result_clean\n",
    "            \n",
    "            # Fuzzy matching for partial responses\n",
    "            for topic in available_topics:\n",
    "                if topic.lower() in result_clean.lower() or result_clean.lower() in topic.lower():\n",
    "                    if len(result_clean) > 5:\n",
    "                        return topic\n",
    "            \n",
    "            # Keyword fallback strategies\n",
    "            question_lower = question.lower()\n",
    "            if \"religion\" in question_lower and \"course\" in question_lower:\n",
    "                return \"Religion Course Registration and Requirements\"\n",
    "            elif \"scholarship\" in question_lower or \"heber\" in question_lower:\n",
    "                return \"Heber J. Grant Scholarship Application Process\"\n",
    "            elif (\"byu pathway\" in question_lower or \"accredited\" in question_lower):\n",
    "                return \"BYU-Pathway Program\"\n",
    "            elif (\"english connect 3\" in question_lower or \"ec3\" in question_lower):\n",
    "                return \"English Connect 3 Registration Process\"\n",
    "            elif \"certificate\" in question_lower and (\"completion\" in question_lower or \"complete\" in question_lower):\n",
    "                return \"Certificate Application Process\"\n",
    "            \n",
    "            return \"Other\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error classifying question: {e}\")\n",
    "            return \"Other\"\n",
    "\n",
    "    async def worker_best_model(question_data: Dict, sem: asyncio.Semaphore, available_topics: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Worker using the best model\"\"\"\n",
    "        async with sem:\n",
    "            question = question_data['question']\n",
    "            correct_topic = question_data['cluster_topic']\n",
    "            \n",
    "            openai_topic = await classify_with_best_model(question, available_topics)\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'correct_topic': correct_topic,\n",
    "                'openai_topic': openai_topic,\n",
    "                'is_correct': openai_topic == correct_topic\n",
    "            }\n",
    "\n",
    "    # Run full test with best model\n",
    "    all_questions_data = df_questions_topics.to_dict('records')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sem = asyncio.Semaphore(OPTIMAL_CONCURRENCY)  # Use optimal concurrency\n",
    "    tasks = [asyncio.create_task(worker_best_model(q_data, sem, topics_list)) for q_data in all_questions_data]\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(all_questions_data)} questions with {BEST_MODEL}...\")\n",
    "    best_model_results = await asyncio.gather(*tasks)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate final results\n",
    "    total_questions = len(best_model_results)\n",
    "    correct_count = sum(1 for result in best_model_results if result['is_correct'])\n",
    "    final_accuracy = (correct_count / total_questions) * 100\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"üèÜ FINAL RESULTS WITH {BEST_MODEL}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Total questions: {total_questions}\")\n",
    "    print(f\"‚úÖ Correct: {correct_count}\")\n",
    "    print(f\"‚ùå Incorrect: {total_questions - correct_count}\")\n",
    "    print(f\"üéØ Accuracy: {final_accuracy:.1f}%\")\n",
    "    print(f\"‚ö° Processing time: {processing_time:.1f} seconds\")\n",
    "    print(f\"üìà Questions per second: {total_questions / processing_time:.1f}\")\n",
    "\n",
    "    # Decision based on results\n",
    "    if final_accuracy >= 90:\n",
    "        print(f\"\\nüéâ EXCELLENT! {BEST_MODEL} achieved {final_accuracy:.1f}% accuracy (‚â•90%)\")\n",
    "        print(\"‚úÖ RECOMMENDATION: Use OpenAI classification with this model (skip clustering)\")\n",
    "    elif final_accuracy >= 80:\n",
    "        print(f\"\\nüåü VERY GOOD! {BEST_MODEL} achieved {final_accuracy:.1f}% accuracy\")\n",
    "        print(\"‚ö° RECOMMENDATION: Consider hybrid approach or minor refinements\")\n",
    "    elif final_accuracy >= 70:\n",
    "        print(f\"\\nüìà GOOD PROGRESS! {BEST_MODEL} achieved {final_accuracy:.1f}% accuracy\")\n",
    "        print(\"üîß RECOMMENDATION: Continue refining or use hybrid approach\")\n",
    "    else:\n",
    "        print(f\"\\nüîÑ NEEDS MORE WORK: {BEST_MODEL} achieved {final_accuracy:.1f}% accuracy\")\n",
    "        print(\"üõ†Ô∏è  RECOMMENDATION: Stick with clustering approach\")\n",
    "\n",
    "    # Compare with previous best result\n",
    "    print(f\"\\nüìà COMPARISON:\")\n",
    "    print(f\"   Previous best (gpt-4o-mini): 53.2%\")\n",
    "    print(f\"   {BEST_MODEL}: {final_accuracy:.1f}%\")\n",
    "    improvement = final_accuracy - 53.2\n",
    "    print(f\"   Improvement: {improvement:+.1f} percentage points\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No best model identified. Please run the model testing cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-5 Nano Optimized Test\n",
    "if 'best_model' in locals() and best_model and best_model.startswith('gpt-5'):\n",
    "    print(f\"\\nüöÄ GPT-5 NANO OPTIMIZED TEST\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Running optimized test with {best_model} using correct GPT-5 parameters...\")\n",
    "    print(f\"üìä Testing on {len(df_questions_topics)} questions\")\n",
    "    print(f\"‚öôÔ∏è  Using max_completion_tokens (not max_tokens) and no temperature parameter\")\n",
    "    \n",
    "    async def classify_with_gpt5_nano(question: str, available_topics: List[str]) -> str:\n",
    "        \"\"\"GPT-5 nano optimized classification function\"\"\"\n",
    "        topics_str = \"\\n\".join([f\"‚Ä¢ {topic}\" for topic in available_topics])\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert at classifying BYU-Pathway student questions into specific topics.\n",
    "\n",
    "AVAILABLE TOPICS:\n",
    "{topics_str}\n",
    "‚Ä¢ Other (only if nothing else fits)\n",
    "\n",
    "CLASSIFICATION RULES:\n",
    "1. Look for keywords and context in the question\n",
    "2. Choose the MOST SPECIFIC topic that matches\n",
    "3. Key patterns:\n",
    "   - \"religion courses\" ‚Üí Religion Course Registration and Requirements\n",
    "   - \"certificate\" or \"completion\" ‚Üí Certificate Application Process  \n",
    "   - \"scholarship\" or \"Heber\" ‚Üí Heber J. Grant Scholarship Application Process\n",
    "   - \"BYU Pathway\" or \"accredited\" ‚Üí BYU-Pathway Program\n",
    "   - \"English Connect 3\" or \"EC3\" ‚Üí English Connect 3 Registration Process\n",
    "4. Avoid \"Other\" unless truly nothing fits\n",
    "\n",
    "QUESTION: \"{question}\"\n",
    "\n",
    "Respond with ONLY the exact topic name from the list above:\"\"\"\n",
    "\n",
    "        try:\n",
    "            # GPT-5 nano specific parameters\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=best_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert classifier. Respond with only the exact topic name.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_completion_tokens=200  # Increased for longer responses\n",
    "                # No temperature parameter as it's not supported\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip()\n",
    "            result_clean = result.replace(\"‚Ä¢\", \"\").replace(\"-\", \"\").strip().rstrip('.')\n",
    "            \n",
    "            # Direct match\n",
    "            if result_clean in available_topics:\n",
    "                return result_clean\n",
    "            \n",
    "            # Fuzzy matching for partial responses\n",
    "            for topic in available_topics:\n",
    "                if topic.lower() in result_clean.lower() or result_clean.lower() in topic.lower():\n",
    "                    if len(result_clean) > 5:\n",
    "                        return topic\n",
    "            \n",
    "            # Keyword fallback strategies\n",
    "            question_lower = question.lower()\n",
    "            if \"religion\" in question_lower and \"course\" in question_lower:\n",
    "                return \"Religion Course Registration and Requirements\"\n",
    "            elif \"scholarship\" in question_lower or \"heber\" in question_lower:\n",
    "                return \"Heber J. Grant Scholarship Application Process\"\n",
    "            elif (\"byu pathway\" in question_lower or \"accredited\" in question_lower):\n",
    "                return \"BYU-Pathway Program\"\n",
    "            elif (\"english connect 3\" in question_lower or \"ec3\" in question_lower):\n",
    "                return \"English Connect 3 Registration Process\"\n",
    "            elif \"certificate\" in question_lower and (\"completion\" in question_lower or \"complete\" in question_lower):\n",
    "                return \"Certificate Application Process\"\n",
    "            \n",
    "            return \"Other\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error classifying question: {e}\")\n",
    "            return \"Other\"\n",
    "\n",
    "    async def worker_gpt5_nano(question_data: Dict, sem: asyncio.Semaphore, available_topics: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Worker optimized for GPT-5 nano\"\"\"\n",
    "        async with sem:\n",
    "            question = question_data['question']\n",
    "            correct_topic = question_data['cluster_topic']\n",
    "            \n",
    "            openai_topic = await classify_with_gpt5_nano(question, available_topics)\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'correct_topic': correct_topic,\n",
    "                'openai_topic': openai_topic,\n",
    "                'is_correct': openai_topic == correct_topic\n",
    "            }\n",
    "\n",
    "    # Run optimized test\n",
    "    all_questions_data = df_questions_topics.to_dict('records')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sem = asyncio.Semaphore(OPTIMAL_CONCURRENCY)  # Use optimal concurrency\n",
    "    tasks = [asyncio.create_task(worker_gpt5_nano(q_data, sem, topics_list)) for q_data in all_questions_data]\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(all_questions_data)} questions with {best_model}...\")\n",
    "    gpt5_nano_results = await asyncio.gather(*tasks)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate final results\n",
    "    total_questions = len(gpt5_nano_results)\n",
    "    correct_count = sum(1 for result in gpt5_nano_results if result['is_correct'])\n",
    "    gpt5_accuracy = (correct_count / total_questions) * 100\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"üèÜ GPT-5 NANO RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Total questions: {total_questions}\")\n",
    "    print(f\"‚úÖ Correct: {correct_count}\")\n",
    "    print(f\"‚ùå Incorrect: {total_questions - correct_count}\")\n",
    "    print(f\"üéØ Accuracy: {gpt5_accuracy:.1f}%\")\n",
    "    print(f\"‚ö° Processing time: {processing_time:.1f} seconds\")\n",
    "    print(f\"üìà Questions per second: {total_questions / processing_time:.1f}\")\n",
    "    print(f\"üí∞ Cost efficiency: {gpt5_accuracy:.1f}% accuracy at $0.05/$0.40 per 1M tokens\")\n",
    "\n",
    "    # Decision based on results\n",
    "    if gpt5_accuracy >= 90:\n",
    "        print(f\"\\nüéâ EXCELLENT! GPT-5 nano achieved {gpt5_accuracy:.1f}% accuracy (‚â•90%)\")\n",
    "        print(\"‚úÖ RECOMMENDATION: Use GPT-5 nano for production (skip clustering)\")\n",
    "        print(\"üí° Benefits: Highest accuracy + lowest cost + fastest speed\")\n",
    "    elif gpt5_accuracy >= 80:\n",
    "        print(f\"\\nüåü VERY GOOD! GPT-5 nano achieved {gpt5_accuracy:.1f}% accuracy\")\n",
    "        print(\"‚ö° RECOMMENDATION: Consider GPT-5 nano with minor refinements\")\n",
    "    elif gpt5_accuracy >= 70:\n",
    "        print(f\"\\nüìà GOOD PROGRESS! GPT-5 nano achieved {gpt5_accuracy:.1f}% accuracy\")\n",
    "        print(\"üîß RECOMMENDATION: Continue refining or use hybrid approach\")\n",
    "    else:\n",
    "        print(f\"\\nüîÑ NEEDS MORE WORK: GPT-5 nano achieved {gpt5_accuracy:.1f}% accuracy\")\n",
    "        print(\"üõ†Ô∏è  RECOMMENDATION: Consider other approaches\")\n",
    "\n",
    "    # Compare with previous results\n",
    "    print(f\"\\nüìà COMPREHENSIVE COMPARISON:\")\n",
    "    print(f\"   GPT-4o-mini baseline: 53.2%\")\n",
    "    print(f\"   GPT-5 nano (optimized): {gpt5_accuracy:.1f}%\")\n",
    "    improvement = gpt5_accuracy - 53.2\n",
    "    print(f\"   Improvement: {improvement:+.1f} percentage points\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(f\"\\nüí° GPT-5 nano is {'SIGNIFICANTLY' if improvement > 20 else 'MODERATELY' if improvement > 10 else 'SLIGHTLY'} better!\")\n",
    "        print(f\"   + Higher accuracy: {improvement:+.1f}%\")\n",
    "        print(f\"   + Lower cost: $0.05 vs $0.10+ input cost\")\n",
    "        print(f\"   + Faster speed: Optimized for classification\")\n",
    "        print(f\"   + Larger context: 400K tokens vs 128K\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  GPT-5 nano test skipped - model not available or not selected as best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-5 Nano Simplified Test (Minimal Prompt for Better Performance)\n",
    "if 'best_model' in locals() and best_model and best_model.startswith('gpt-5'):\n",
    "    print(f\"\\nüöÄ GPT-5 NANO SIMPLIFIED TEST\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Testing {best_model} with simplified prompt for better accuracy...\")\n",
    "    print(f\"üìä Testing on {len(df_questions_topics)} questions\")\n",
    "    print(f\"üéØ Using minimal prompt optimized for classification models\")\n",
    "    \n",
    "    async def classify_simplified_gpt5(question: str, available_topics: List[str]) -> str:\n",
    "        \"\"\"Simplified GPT-5 nano classification - minimal prompt for best performance\"\"\"\n",
    "        \n",
    "        # Create a simpler, more direct prompt\n",
    "        topics_numbered = \"\\n\".join([f\"{i+1}. {topic}\" for i, topic in enumerate(available_topics)])\n",
    "        \n",
    "        prompt = f\"\"\"Classify this student question into one of these topics:\n",
    "\n",
    "{topics_numbered}\n",
    "{len(available_topics) + 1}. Other\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Respond with only the number and topic name (e.g., \"1. Course Registration\"):\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=best_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_completion_tokens=50  # Smaller since we just need topic name\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Extract topic from numbered response\n",
    "            # Look for patterns like \"1. Topic Name\" or just \"Topic Name\"\n",
    "            for i, topic in enumerate(available_topics):\n",
    "                if f\"{i+1}.\" in result or topic.lower() in result.lower():\n",
    "                    return topic\n",
    "            \n",
    "            # Fallback - try fuzzy matching\n",
    "            for topic in available_topics:\n",
    "                if any(word in topic.lower() for word in result.lower().split() if len(word) > 3):\n",
    "                    return topic\n",
    "            \n",
    "            # If \"other\" is mentioned\n",
    "            if \"other\" in result.lower():\n",
    "                return \"Other\"\n",
    "            \n",
    "            # Last resort keyword matching\n",
    "            question_lower = question.lower()\n",
    "            if \"religion\" in question_lower and \"course\" in question_lower:\n",
    "                return \"Religion Course Registration and Requirements\"\n",
    "            elif \"scholarship\" in question_lower or \"heber\" in question_lower:\n",
    "                return \"Heber J. Grant Scholarship Application Process\"\n",
    "            elif \"certificate\" in question_lower:\n",
    "                return \"Certificate Application Process\"\n",
    "            \n",
    "            return \"Other\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"max_tokens\" in str(e) or \"output limit\" in str(e):\n",
    "                # Try even simpler approach for problematic questions\n",
    "                try:\n",
    "                    simple_prompt = f\"Topic for: {question[:100]}?\\n\\nOptions: {', '.join(available_topics[:5])}...\"\n",
    "                    response = await async_client.chat.completions.create(\n",
    "                        model=best_model,\n",
    "                        messages=[{\"role\": \"user\", \"content\": simple_prompt}],\n",
    "                        max_completion_tokens=20\n",
    "                    )\n",
    "                    result = response.choices[0].message.content.strip()\n",
    "                    \n",
    "                    # Find best match\n",
    "                    for topic in available_topics:\n",
    "                        if topic.lower() in result.lower():\n",
    "                            return topic\n",
    "                    return \"Other\"\n",
    "                except:\n",
    "                    return \"Other\"\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Error: {str(e)[:100]}...\")\n",
    "                return \"Other\"\n",
    "\n",
    "    async def worker_simplified_gpt5(question_data: Dict, sem: asyncio.Semaphore, available_topics: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Simplified worker for GPT-5 nano\"\"\"\n",
    "        async with sem:\n",
    "            question = question_data['question']\n",
    "            correct_topic = question_data['cluster_topic']\n",
    "            \n",
    "            openai_topic = await classify_simplified_gpt5(question, available_topics)\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'correct_topic': correct_topic,\n",
    "                'openai_topic': openai_topic,\n",
    "                'is_correct': openai_topic == correct_topic\n",
    "            }\n",
    "\n",
    "    # Run simplified test\n",
    "    all_questions_data = df_questions_topics.to_dict('records')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sem = asyncio.Semaphore(OPTIMAL_CONCURRENCY)\n",
    "    tasks = [asyncio.create_task(worker_simplified_gpt5(q_data, sem, topics_list)) for q_data in all_questions_data]\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(all_questions_data)} questions with simplified approach...\")\n",
    "    simplified_results = await asyncio.gather(*tasks)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate results\n",
    "    total_questions = len(simplified_results)\n",
    "    correct_count = sum(1 for result in simplified_results if result['is_correct'])\n",
    "    simplified_accuracy = (correct_count / total_questions) * 100\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"üèÜ GPT-5 NANO SIMPLIFIED RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Total questions: {total_questions}\")\n",
    "    print(f\"‚úÖ Correct: {correct_count}\")\n",
    "    print(f\"‚ùå Incorrect: {total_questions - correct_count}\")\n",
    "    print(f\"üéØ Accuracy: {simplified_accuracy:.1f}%\")\n",
    "    print(f\"‚ö° Processing time: {processing_time:.1f} seconds\")\n",
    "    print(f\"üìà Questions per second: {total_questions / processing_time:.1f}\")\n",
    "    print(f\"üí∞ Estimated cost: ~${(total_questions * 50 / 1000000) * 0.05:.4f} (very low!)\")\n",
    "\n",
    "    # Analysis and recommendations\n",
    "    if simplified_accuracy >= 90:\n",
    "        print(f\"\\nüéâ OUTSTANDING! GPT-5 nano achieved {simplified_accuracy:.1f}% accuracy (‚â•90%)\")\n",
    "        print(\"‚úÖ RECOMMENDATION: Use GPT-5 nano for production (skip clustering entirely)\")\n",
    "        print(\"üöÄ Benefits:\")\n",
    "        print(f\"   ‚Ä¢ Highest accuracy: {simplified_accuracy:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Lowest cost: $0.05 input (10x cheaper than GPT-4)\")\n",
    "        print(f\"   ‚Ä¢ Fastest speed: {total_questions / processing_time:.1f} q/sec\")\n",
    "        print(f\"   ‚Ä¢ Largest context: 400K tokens\")\n",
    "    elif simplified_accuracy >= 80:\n",
    "        print(f\"\\nüåü EXCELLENT! GPT-5 nano achieved {simplified_accuracy:.1f}% accuracy\")\n",
    "        print(\"‚ö° RECOMMENDATION: Use GPT-5 nano with confidence\")\n",
    "    elif simplified_accuracy >= 70:\n",
    "        print(f\"\\nüìà VERY GOOD! GPT-5 nano achieved {simplified_accuracy:.1f}% accuracy\")\n",
    "        print(\"üîß RECOMMENDATION: Consider refinements or hybrid approach\")\n",
    "    else:\n",
    "        print(f\"\\nüîÑ GOOD PROGRESS: GPT-5 nano achieved {simplified_accuracy:.1f}% accuracy\")\n",
    "        print(\"üõ†Ô∏è  RECOMMENDATION: Continue optimizing or consider alternatives\")\n",
    "\n",
    "    # Final comparison\n",
    "    print(f\"\\nüìà FINAL COMPARISON CHART:\")\n",
    "    print(f\"   GPT-4o-mini baseline:     53.2%\")\n",
    "    print(f\"   GPT-5 nano (simplified):  {simplified_accuracy:.1f}%\")\n",
    "    improvement = simplified_accuracy - 53.2\n",
    "    print(f\"   Net improvement:          {improvement:+.1f} percentage points\")\n",
    "    \n",
    "    if improvement > 30:\n",
    "        print(f\"\\nüèÜ BREAKTHROUGH RESULT! GPT-5 nano is dramatically better!\")\n",
    "    elif improvement > 20:\n",
    "        print(f\"\\nüéØ EXCELLENT IMPROVEMENT! GPT-5 nano significantly outperforms baseline!\")\n",
    "    elif improvement > 10:\n",
    "        print(f\"\\nüìà GOOD IMPROVEMENT! GPT-5 nano shows solid gains!\")\n",
    "    elif improvement > 0:\n",
    "        print(f\"\\n‚úÖ POSITIVE RESULT! GPT-5 nano performs better than baseline!\")\n",
    "    \n",
    "    print(\"\\nüí° FINAL DECISION:\")\n",
    "    if simplified_accuracy >= 85:\n",
    "        print(\"   ‚Üí IMPLEMENT GPT-5 nano classification in production\")\n",
    "        print(\"   ‚Üí Skip clustering step entirely\")\n",
    "        print(\"   ‚Üí Massive cost savings + higher accuracy\")\n",
    "    elif simplified_accuracy >= 75:\n",
    "        print(\"   ‚Üí Consider GPT-5 nano for production with monitoring\")\n",
    "        print(\"   ‚Üí May still benefit from some prompt refinements\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Continue with clustering approach\")\n",
    "        print(\"   ‚Üí GPT-5 nano needs more optimization for this task\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  GPT-5 nano simplified test skipped - not using GPT-5 model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Test GPT-5 Nano Specifically (Optimized for Classification)\n",
    "print(\"üöÄ TESTING GPT-5 NANO AVAILABILITY\")\n",
    "print(\"=\"*50)\n",
    "print(\"üìã GPT-5 Nano Specifications:\")\n",
    "print(\"   ‚Ä¢ Optimized for: Classification and summarization tasks\")\n",
    "print(\"   ‚Ä¢ Speed: Very fast\")\n",
    "print(\"   ‚Ä¢ Price: $0.05 input / $0.40 output per 1M tokens\")\n",
    "print(\"   ‚Ä¢ Context: 400,000 tokens\")\n",
    "print(\"   ‚Ä¢ Output: 128,000 max tokens\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "async def test_gpt5_nano():\n",
    "    \"\"\"Test if GPT-5 nano is available and working\"\"\"\n",
    "    models_to_test = [\n",
    "        \"gpt-5-nano\",\n",
    "        \"gpt-5-nano-2025-08-07\",  # Specific snapshot version\n",
    "    ]\n",
    "    \n",
    "    for model in models_to_test:\n",
    "        print(f\"\\nüîç Testing {model}...\")\n",
    "        \n",
    "        # GPT-5 nano parameter combinations to try\n",
    "        parameter_sets = [\n",
    "            # Try with minimal parameters\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": \"Test classification\"}]},\n",
    "            # Try with temperature=1 (default)\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": \"Test\"}], \"temperature\": 1},\n",
    "            # Try without temperature\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]},\n",
    "            # Try with different temperature values\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": \"Test\"}], \"temperature\": 0.1},\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": \"Test\"}], \"temperature\": 0.5},\n",
    "        ]\n",
    "        \n",
    "        for i, params in enumerate(parameter_sets):\n",
    "            try:\n",
    "                print(f\"   üîß Trying parameter set {i+1}...\")\n",
    "                response = await async_client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    **params\n",
    "                )\n",
    "                result = response.choices[0].message.content.strip()\n",
    "                print(f\"   ‚úÖ {model} is AVAILABLE!\")\n",
    "                print(f\"   üìù Test response: '{result}'\")\n",
    "                print(f\"   üéØ Working parameters: {params}\")\n",
    "                return model  # Return the working model\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                if \"does not exist\" in error_msg:\n",
    "                    print(f\"   ‚ùå {model}: Model not found in your account\")\n",
    "                    break  # No point trying other parameter sets\n",
    "                elif \"temperature\" in error_msg.lower():\n",
    "                    print(f\"   ‚ö†Ô∏è  Parameter set {i+1}: Temperature issue - {error_msg[:100]}\")\n",
    "                    continue  # Try next parameter set\n",
    "                elif \"max_tokens\" in error_msg.lower():\n",
    "                    print(f\"   ‚ö†Ô∏è  Parameter set {i+1}: Max tokens issue - {error_msg[:100]}\")\n",
    "                    continue  # Try next parameter set\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  Parameter set {i+1}: {error_msg[:100]}\")\n",
    "                    continue  # Try next parameter set\n",
    "        \n",
    "        print(f\"   ‚ùå {model}: All parameter combinations failed\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test GPT-5 nano availability\n",
    "available_gpt5_nano = await test_gpt5_nano()\n",
    "\n",
    "if available_gpt5_nano:\n",
    "    print(f\"\\nüéâ GPT-5 NANO IS AVAILABLE: {available_gpt5_nano}\")\n",
    "    print(\"üöÄ Let's run the full test with GPT-5 nano!\")\n",
    "    \n",
    "    # Override the best_model with GPT-5 nano for testing\n",
    "    best_model = available_gpt5_nano\n",
    "    BEST_MODEL = available_gpt5_nano\n",
    "    \n",
    "    print(f\"\\nüèÜ UPDATED BEST MODEL: {BEST_MODEL}\")\n",
    "    print(\"   This model is specifically optimized for classification tasks!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nüí° GPT-5 nano not yet available in your account\")\n",
    "    print(f\"   Your account may not have access to GPT-5 models yet\")\n",
    "    print(f\"   Continuing with current best model: gpt-4o-mini\")\n",
    "    \n",
    "    # Keep the existing best model\n",
    "    if 'best_model' not in locals():\n",
    "        best_model = \"gpt-4o-mini\"  # Fallback to known working model\n",
    "        BEST_MODEL = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "### What We Tested:\n",
    "1. **Current Process**: Questions ‚Üí Embeddings ‚Üí Clustering ‚Üí Natural Language Topics\n",
    "2. **Proposed Process**: Questions + Topic List ‚Üí Direct OpenAI Classification (skip clustering)\n",
    "\n",
    "### Key Findings:\n",
    "- **Optimal Model**: GPT-4o-mini (better reliability than GPT-4o)\n",
    "- **Optimal Concurrency**: 16 parallel requests (balance of speed and API stability)\n",
    "- **Best Approach**: Comprehensive prompt with keyword fallbacks\n",
    "\n",
    "### Decision Framework:\n",
    "- **If accuracy ‚â• 90%**: Skip clustering, use direct OpenAI classification\n",
    "- **If 80-89% accuracy**: Consider hybrid approach (OpenAI + clustering backup)\n",
    "- **If < 80% accuracy**: Continue with clustering approach\n",
    "\n",
    "### Next Steps:\n",
    "Based on the test results above, implement the appropriate approach in production."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04989cc1d1624c1c8e072e10913af90a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30980c5f49d745c3b7b6ab08364c3141": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ecfe09d11ab47fe9be38892c1d79ded": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60e546b611694f3f8afbe3618da0a73b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8b921a1591c4b5eac40f60d88d61c13",
       "IPY_MODEL_ba7fa2d943dd4b5e92399bab17fd6130",
       "IPY_MODEL_806b413bf955483393b3e1c23e3fd3af"
      ],
      "layout": "IPY_MODEL_b2e04707dd2240bb806a78d8ffa9d9bb"
     }
    },
    "806b413bf955483393b3e1c23e3fd3af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04989cc1d1624c1c8e072e10913af90a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4ecfe09d11ab47fe9be38892c1d79ded",
      "value": "‚Äá3/3‚Äá[00:09&lt;00:00,‚Äá‚Äá3.23s/it]"
     }
    },
    "a18a941503834394a0f652f841d53515": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2e04707dd2240bb806a78d8ffa9d9bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8aa840aff79472fb1c2ec663eceb6f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba7fa2d943dd4b5e92399bab17fd6130": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a18a941503834394a0f652f841d53515",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b8aa840aff79472fb1c2ec663eceb6f6",
      "value": 3
     }
    },
    "c8b921a1591c4b5eac40f60d88d61c13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff62af236d7847f5b48f158e2db4ce36",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_30980c5f49d745c3b7b6ab08364c3141",
      "value": "100%"
     }
    },
    "ff62af236d7847f5b48f158e2db4ce36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
